from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType
from bs4 import BeautifulSoup
import json
import re
import logging
import schedule
import time
from datetime import datetime, timedelta
import os
import telegram
import asyncio

# ============================================================
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ùˆ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ
# ============================================================
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Ù…Ø®ÙÛŒ Ú©Ø±Ø¯Ù† Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ HTTP ØªÙ„Ú¯Ø±Ø§Ù… Ùˆ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡
logging.getLogger('httpx').setLevel(logging.CRITICAL)  # ÙÙ‚Ø· Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ
logging.getLogger('telegram').setLevel(logging.ERROR)  # ÙÙ‚Ø· Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù…Ù‡Ù…
logging.getLogger('httpcore').setLevel(logging.CRITICAL)
logging.getLogger('selenium').setLevel(logging.WARNING)
logging.getLogger('urllib3').setLevel(logging.WARNING)

BOT_TOKEN = "7697466323:AAFXXszQt_lAPn4qCefx3VnnZYVhTuQiuno"

# Ù„ÛŒØ³Øª Ø³ÙÛŒØ¯ Ú©Ø´ÙˆØ±Ù‡Ø§ Ùˆ Ù„ÛŒÚ¯â€ŒÙ‡Ø§
WHITELIST = {
    "Ø§Ù†Ú¯Ù„ÛŒØ³": ["Ù„ÛŒÚ¯ Ø¨Ø±ØªØ± Ø§Ù†Ú¯Ù„ÛŒØ³", "Ø¬Ø§Ù… Ø­Ø°ÙÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³", "Ú†Ù…Ù¾ÛŒÙˆÙ†Ø´ÛŒÙ¾ Ø§Ù†Ú¯Ù„ÛŒØ³", "Ø¬Ø§Ù… Ø§ØªØ­Ø§Ø¯ÛŒÙ‡ Ø§Ù†Ú¯Ù„ÛŒØ³", "Ø³ÙˆÙ¾Ø±Ø¬Ø§Ù… Ø§Ù†Ú¯Ù„ÛŒØ³ (Ø¬Ø§Ù… Ø®ÛŒØ±ÛŒÙ‡)"],
    "Ø§Ø±ÙˆÙ¾Ø§": ["Ù„ÛŒÚ¯ Ù‚Ù‡Ø±Ù…Ø§Ù†Ø§Ù† Ø§Ø±ÙˆÙ¾Ø§", "Ù„ÛŒÚ¯ Ø§Ø±ÙˆÙ¾Ø§", "Ù„ÛŒÚ¯ Ú©Ù†ÙØ±Ø§Ù†Ø³ Ø§Ø±ÙˆÙ¾Ø§", "Ø³ÙˆÙ¾Ø± Ø¬Ø§Ù… Ø§Ø±ÙˆÙ¾Ø§"],
    "Ø¢Ø³ÛŒØ§": ["Ù„ÛŒÚ¯ Ù†Ø®Ø¨Ú¯Ø§Ù† Ø¢Ø³ÛŒØ§", "Ù„ÛŒÚ¯ Ù‚Ù‡Ø±Ù…Ø§Ù†Ø§Ù† Ø¢Ø³ÛŒØ§ Û²"],
    "Ø§ÛŒØªØ§Ù„ÛŒØ§": ["Ø³Ø±ÛŒ Ø¢ Ø§ÛŒØªØ§Ù„ÛŒØ§", "Ø¬Ø§Ù… Ø­Ø°ÙÛŒ Ø§ÛŒØªØ§Ù„ÛŒØ§", "Ø³ÙˆÙ¾Ø± Ø¬Ø§Ù… Ø§ÛŒØªØ§Ù„ÛŒØ§"],
    "Ø§Ø³Ù¾Ø§Ù†ÛŒØ§": ["Ù„Ø§Ù„ÛŒÚ¯Ø§ Ø§Ø³Ù¾Ø§Ù†ÛŒØ§", "Ú©ÙˆÙ¾Ø§ Ø¯Ù„ Ø±ÛŒ Ø§Ø³Ù¾Ø§Ù†ÛŒØ§"],
    "Ø¢Ù„Ù…Ø§Ù†": ["Ø¨ÙˆÙ†Ø¯Ø³â€ŒÙ„ÛŒÚ¯Ø§ Ø¢Ù„Ù…Ø§Ù†", "Ø¬Ø§Ù… Ø­Ø°ÙÛŒ Ø¢Ù„Ù…Ø§Ù†"],
    "ÙØ±Ø§Ù†Ø³Ù‡": ["Ù„ÛŒÚ¯ Û± ÙØ±Ø§Ù†Ø³Ù‡", "Ø¬Ø§Ù… Ø­Ø°ÙÛŒ ÙØ±Ø§Ù†Ø³Ù‡"],
    "Ø¨Ø±Ø²ÛŒÙ„": ["Ø³Ø±ÛŒ Ø¢ Ø¨Ø±Ø²ÛŒÙ„"],
    "Ø¹Ø±Ø¨Ø³ØªØ§Ù† Ø³Ø¹ÙˆØ¯ÛŒ": ["Ù„ÛŒÚ¯ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¹Ø±Ø¨Ø³ØªØ§Ù† Ø³Ø¹ÙˆØ¯ÛŒ"],
    "ØªØ±Ú©ÛŒÙ‡": ["Ø³ÙˆÙ¾Ø± Ù„ÛŒÚ¯ ØªØ±Ú©ÛŒÙ‡"],
    "Ù‡Ù„Ù†Ø¯": ["Ù„ÛŒÚ¯ Ø¨Ø±ØªØ± Ù‡Ù„Ù†Ø¯", "Ø¬Ø§Ù… Ø­Ø°ÙÛŒ Ù‡Ù„Ù†Ø¯"],
    "Ù¾Ø±ØªØºØ§Ù„": ["Ù„ÛŒÚ¯ Ø¨Ø±ØªØ± Ù¾Ø±ØªØºØ§Ù„", "Ø¬Ø§Ù… Ø­Ø°ÙÛŒ Ù¾Ø±ØªØºØ§Ù„"],
    "Ø§Ù†Ø¯ÙˆÙ†Ø²ÛŒ": ["Ø³ÙˆÙ¾Ø± Ù„ÛŒÚ¯ Ø§Ù†Ø¯ÙˆÙ†Ø²ÛŒ"],
}

# Ù†Ø±Ù…Ø§Ù„ Ø³Ø§Ø²ÛŒ WHITELIST
WHITELIST = {k.strip(): [l.strip() for l in v] for k, v in WHITELIST.items()}

# ============================================================
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ
# ============================================================
def normalize(s):
    """Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ"""
    return " ".join(s.split()).strip() if s else ""

def get_circle_color(odds):
    """ØªØ¹ÛŒÛŒÙ† Ø±Ù†Ú¯ Ø¯Ø§ÛŒØ±Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¶Ø±ÛŒØ¨"""
    if odds <= 1.2: return "ğŸŸ¢"
    if odds <= 1.4: return "ğŸŸ¡"
    if odds <= 1.6: return "ğŸŸ "
    return "âšª"

def get_score_circle(score_diff):
    """ØªØ¹ÛŒÛŒÙ† Ø±Ù†Ú¯ Ø¯Ø§ÛŒØ±Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø®ØªÙ„Ø§Ù Ú¯Ù„"""
    if score_diff > 1: return "ğŸŸ¢"
    if score_diff == 1: return "ğŸŸ¡"
    return "âšª"

def is_whitelisted(country, league):
    """Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù…Ø³Ø§Ø¨Ù‚Ù‡ Ø¯Ø± Ù„ÛŒØ³Øª Ø³ÙÛŒØ¯ Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±"""
    country_norm = normalize(country)
    league_norm = normalize(league)
    return country_norm in WHITELIST and league_norm in WHITELIST.get(country_norm, [])

def load_json(filename):
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ JSON"""
    if os.path.exists(filename):
        try:
            with open(filename, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logging.error(f"Error loading {filename}: {e}")
    return []

def save_json(data, filename):
    """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ JSON"""
    try:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        logging.info(f"Data saved to {filename}")
    except IOError as e:
        logging.error(f"Error saving {filename}: {e}")

# ============================================================
# Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±
# ============================================================
def setup_driver():
    """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Chrome Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡"""
    opts = Options()
    for arg in ["--headless", "--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu",
                "--disable-extensions", "--disable-software-rasterizer", "--disable-background-networking",
                "--window-size=1920x1080", "--blink-settings=imagesEnabled=false",
                "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "accept-language=fa-IR,fa;q=0.9"]:
        opts.add_argument(arg)
    
    opts.add_experimental_option("prefs", {"profile.default_content_setting_values": {"images": 2, "stylesheets": 2}})
    
    # ØªØ´Ø®ÛŒØµ Ù…Ø­ÛŒØ· Termux
    termux_chrome = "/data/data/com.termux/files/usr/bin/chromium-browser"
    termux_chromedriver = "/data/data/com.termux/files/usr/bin/chromedriver"
    
    try:
        # Ø§Ú¯Ø± Ø¯Ø± Termux Ù‡Ø³ØªÛŒÙ…
        if os.path.exists(termux_chrome):
            logging.info("ğŸ”§ Detected Termux environment")
            opts.binary_location = termux_chrome
            
            # Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ chromedriver Ø§Ø² Termux Ù†ØµØ¨ Ø´Ø¯Ù‡
            if os.path.exists(termux_chromedriver):
                logging.info("âœ… Using chromedriver from Termux repository")
                service = Service(executable_path=termux_chromedriver)
                driver = webdriver.Chrome(service=service, options=opts)
            else:
                # Ø§Ú¯Ø± chromedriver Ù†ØµØ¨ Ù†ÛŒØ³ØªØŒ Ø³Ø¹ÛŒ Ø¯Ø± Ù†ØµØ¨ Ø¢Ù†
                logging.warning("âš ï¸  chromedriver not found in Termux")
                logging.info("ğŸ’¡ Installing chromedriver from Termux repository...")
                logging.info("   Please run: pkg install tur-repo && pkg install chromium-chromedriver")
                logging.error("âŒ chromedriver is required but not installed")
                raise FileNotFoundError(
                    "chromedriver not found. Install it with: pkg install tur-repo && pkg install chromium-chromedriver"
                )
            
            logging.info("âœ… Chrome initialized successfully")
        else:
            # Ù…Ø­ÛŒØ· Ø¹Ø§Ø¯ÛŒ (Railway/Windows/Linux)
            logging.info("â³ Initializing Chrome...")
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=opts)
            logging.info("âœ… Chrome initialized successfully")
            
    except FileNotFoundError as e:
        logging.error(f"âŒ {e}")
        raise
    except Exception as e:
        logging.error(f"âŒ Failed to init Chrome: {e}")
        logging.error("\n" + "="*60)
        logging.error("ğŸ’¡ Troubleshooting for Termux users:")
        logging.error("="*60)
        logging.error("1. Install Termux User Repository (TUR):")
        logging.error("   pkg install tur-repo")
        logging.error("")
        logging.error("2. Install chromium and chromedriver from TUR:")
        logging.error("   pkg install chromium chromium-chromedriver")
        logging.error("")
        logging.error("3. Verify installation:")
        logging.error("   which chromium-browser")
        logging.error("   which chromedriver")
        logging.error("")
        logging.error("4. Clear webdriver-manager cache (if exists):")
        logging.error("   rm -rf ~/.wdm")
        logging.error("")
        logging.error("5. Make sure you have enough storage:")
        logging.error("   df -h")
        logging.error("="*60)
        raise
    
    try:
        driver.execute_cdp_cmd("Network.setBlockedURLs", {"urls": ["*.css", "*.jpg", "*.jpeg", "*.png", "*.gif"]})
    except Exception as e:
        logging.warning(f"Could not block URLs: {e}")
    
    return driver

# ============================================================
# Ø§Ø±Ø³Ø§Ù„ Ù‡Ø´Ø¯Ø§Ø± Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…
# ============================================================
async def send_error_alert(error_message):
    """Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… Ø®Ø·Ø§ Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†"""
    bot = telegram.Bot(token=BOT_TOKEN)
    chat_ids = load_json("chat_ids.json")
    
    if not chat_ids:
        logging.warning("âš ï¸ No subscribers to send error alert")
        return
    
    error_msg = f"âš ï¸ <b>Ø®Ø·Ø§ÛŒ Ø³ÛŒØ³ØªÙ…</b>\n\n{error_message}\n\nğŸ• {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    
    logging.info(f"ğŸ“¤ Sending error alert to {len(chat_ids)} chat(s)")
    for chat_id in chat_ids:
        try:
            await bot.send_message(chat_id=chat_id, text=error_msg, parse_mode="HTML")
            await asyncio.sleep(1)
        except Exception as e:
            logging.error(f"âŒ Error sending alert to {chat_id}: {e}")

async def send_alerts(messages):
    """Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù‡Ø´Ø¯Ø§Ø± Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†"""
    bot = telegram.Bot(token=BOT_TOKEN)
    chat_ids = load_json("chat_ids.json")
    logging.info(f"ğŸ“¤ Sending {len(messages)} alert(s) to {len(chat_ids)} chat(s)")
    
    for idx, msg in enumerate(messages, 1):
        logging.info(f"\n{'='*60}\nğŸ“¨ Alert {idx}/{len(messages)}:\n{'='*60}\n{msg}\n{'='*60}")
        for chat_id in chat_ids:
            try:
                await bot.send_message(chat_id=chat_id, text=msg, parse_mode="HTML")
                logging.info(f"âœ… Sent to {chat_id}")
                await asyncio.sleep(2)
            except Exception as e:
                logging.error(f"âŒ Error sending to {chat_id}: {e}")

# ============================================================
# Ø§Ø³Ú©Ø±Ù¾ Ú©Ø±Ø¯Ù† Ø¶Ø±Ø§ÛŒØ¨
# ============================================================
def scrape_odds(driver, url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¶Ø±Ø§ÛŒØ¨ Ù…Ø³Ø§Ø¨Ù‚Ø§Øª (ÙÙ‚Ø· whitelist)"""
    try:
        driver.get(url)
        WebDriverWait(driver, 40).until(EC.presence_of_element_located((By.CLASS_NAME, "c-segment-holder-bc")))
        soup = BeautifulSoup(driver.page_source, "html.parser")
        matches = []
        
        for comp in soup.find_all("div", class_="competition-bc"):
            try:
                titles = comp.find_all("span", class_="c-title-bc ellipsis")
                country = titles[0].text.strip() if len(titles) > 0 else "Unknown"
                league = titles[1].text.strip() if len(titles) > 1 else "Unknown"
                
                # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ whitelist
                if not is_whitelisted(country, league):
                    continue
                
                for match in comp.find_all("div", class_="c-segment-holder-bc single-g-info-bc"):
                    try:
                        teams = match.find_all("span", class_="c-team-info-team-bc team")
                        if len(teams) < 2: continue
                        
                        odds_elems = match.find_all("span", class_="market-odd-bc")
                        if len(odds_elems) < 3: continue
                        
                        matches.append({
                            "home_team": teams[0].text.strip(),
                            "away_team": teams[1].text.strip(),
                            "country": country,
                            "league": league,
                            "odds": {
                                "home_win": odds_elems[0].text.strip(),
                                "draw": odds_elems[1].text.strip(),
                                "away_win": odds_elems[2].text.strip()
                            },
                            "last_updated": datetime.now().isoformat()
                        })
                    except Exception as e:
                        logging.error(f"Error processing match: {e}")
            except Exception as e:
                logging.error(f"Error processing competition: {e}")
        
        return matches
    except Exception as e:
        logging.error(f"Error scraping odds: {e}")
        return []

# ============================================================
# Ø§Ø³Ú©Ø±Ù¾ Ú©Ø±Ø¯Ù† Ù†ØªØ§ÛŒØ¬ Ø²Ù†Ø¯Ù‡
# ============================================================
def scrape_results(driver, url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØªØ§ÛŒØ¬ Ù…Ø³Ø§Ø¨Ù‚Ø§Øª Ø²Ù†Ø¯Ù‡ (ÙÙ‚Ø· whitelist)"""
    try:
        driver.get(url)
        WebDriverWait(driver, 40).until(EC.presence_of_element_located((By.CLASS_NAME, "c-team-info-scores-bc")))
        soup = BeautifulSoup(driver.page_source, "html.parser")
        matches = []
        
        for comp in soup.find_all("div", class_="competition-bc"):
            try:
                titles = comp.find_all("span", class_="c-title-bc ellipsis")
                country = titles[0].text.strip() if len(titles) > 0 else "Unknown"
                league = titles[1].text.strip() if len(titles) > 1 else "Unknown"
                
                # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ whitelist
                if not is_whitelisted(country, league):
                    continue
                
                for match in comp.find_all("div", class_="c-segment-holder-bc single-g-info-bc"):
                    try:
                        teams = match.find_all("span", class_="c-team-info-team-bc team")
                        scores = match.find_all("b", class_="c-team-info-scores-bc")
                        time_info = match.find("span", class_="c-info-score-bc fixed-direction")
                        
                        if len(teams) < 2 or len(scores) < 2: continue
                        
                        minute, status = None, "Unknown"
                        if time_info:
                            time_text = time_info.text.strip()
                            minute_match = re.search(r"(\d+)(?:\s*\+\s*(\d+))?\s*`", time_text)
                            if minute_match:
                                base = int(minute_match.group(1))
                                extra = int(minute_match.group(2)) if minute_match.group(2) else 0
                                minute = f"{base}+{extra}" if extra else str(base)
                                status = "ÙˆÙ‚Øª Ø§Ø¶Ø§ÙÙ‡" if base > 90 or extra else "Ø¯Ø± Ø¬Ø±ÛŒØ§Ù†"
                            else:
                                sibling = match.find("span", class_="c-info-score-bc")
                                status = sibling.text.strip() if sibling else "Unknown"
                        else:
                            status = "Ø´Ø±ÙˆØ¹ Ù†Ø´Ø¯Ù‡"
                        
                        matches.append({
                            "team1": teams[0].text.strip(),
                            "team2": teams[1].text.strip(),
                            "score": {"team1": scores[0].text.strip(), "team2": scores[1].text.strip()},
                            "minute": minute,
                            "status": status,
                            "country": country,
                            "league": league,
                            "last_updated": datetime.now().isoformat()
                        })
                    except Exception as e:
                        logging.error(f"Error processing match: {e}")
            except Exception as e:
                logging.error(f"Error processing competition: {e}")
        
        return matches
    except Exception as e:
        logging.error(f"Error scraping results: {e}")
        return []

# ============================================================
# Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ JSON
# ============================================================
def update_odds_file(new_odds, filename="betforward_odds.json"):
    """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙØ§ÛŒÙ„ Ø¶Ø±Ø§ÛŒØ¨"""
    current = load_json(filename)
    updated = []
    current_time = datetime.now()
    new_set = {(m["home_team"], m["away_team"]) for m in new_odds}
    
    for new_m in new_odds:
        match_id = (new_m["home_team"], new_m["away_team"])
        existing = next((m for m in current if (m["home_team"], m["away_team"]) == match_id), None)
        
        if existing and existing["odds"] != new_m["odds"]:
            new_m["last_updated"] = current_time.isoformat()
            logging.info(f"Updated odds: {match_id[0]} vs {match_id[1]}")
        updated.append(new_m if not existing or existing["odds"] != new_m["odds"] else existing)
    
    for old_m in current:
        match_id = (old_m["home_team"], old_m["away_team"])
        if match_id not in new_set:
            last = datetime.fromisoformat(old_m["last_updated"])
            if current_time - last <= timedelta(hours=3):
                updated.append(old_m)
    
    save_json(updated, filename)

def update_results_file(new_results, filename="betforward_results.json"):
    """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙØ§ÛŒÙ„ Ù†ØªØ§ÛŒØ¬"""
    current = load_json(filename)
    updated = []
    current_time = datetime.now()
    new_set = {(m["team1"], m["team2"]) for m in new_results}
    
    for new_m in new_results:
        updated.append(new_m)
    
    for old_m in current:
        match_id = (old_m["team1"], old_m["team2"])
        if match_id not in new_set:
            last = datetime.fromisoformat(old_m["last_updated"])
            if current_time - last <= timedelta(minutes=30):
                updated.append(old_m)
    
    save_json(updated, filename)

# ============================================================
# ØªÙˆÙ„ÛŒØ¯ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§
# ============================================================
def generate_alert(match, home_odds, away_odds, team_key, team_name, opponent_name, team_odds, team_score, opp_score, minute):
    """ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ§Ù… Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ ÛŒÚ© ØªÛŒÙ…"""
    circle1 = get_circle_color(team_odds)
    circle2 = get_score_circle(opp_score - team_score)
    opp_odds = away_odds if team_key == "team1" else home_odds
    
    return (
        f"{circle1}{circle2} Ù‡Ø´Ø¯Ø§Ø±: Ø¯Ø± Ú©Ø´ÙˆØ± <b>{match['country']}</b> Ø¯Ø± Ù„ÛŒÚ¯ <b>{match['league']}</b> "
        f"{team_name} (Ø¶Ø±ÛŒØ¨: {team_odds}) Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡ {minute or match['status']} "
        f"Ø¨Ø§ Ù†ØªÛŒØ¬Ù‡ {team_score}-{opp_score} Ø§Ø² {opponent_name} (Ø¶Ø±ÛŒØ¨: {opp_odds}) Ø¹Ù‚Ø¨ Ø§Ø³Øª!\n"
        f"ğŸ“ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: 1- Ú©Ø±Ù†Ø± ÛŒØ§ Ø´ÙˆØª Ø²Ø¯Ù† Ù‚ÙˆÛŒ 2- Ú©Ø±Ù†Ø± ÛŒØ§ Ø´ÙˆØª Ù†Ø²Ø¯Ù† Ø¶Ø¹ÛŒÙ 3- Ú¯Ù„ Ø²Ø¯Ù† Ù‚ÙˆÛŒ"
    )

def check_alerts(match, odds_data):
    """Ø¨Ø±Ø±Ø³ÛŒ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù…Ø³Ø§Ø¨Ù‚Ù‡"""
    if not is_whitelisted(match["country"], match["league"]):
        return []
    
    if match["status"] not in ["Ø¯Ø± Ø¬Ø±ÛŒØ§Ù†", "ÙˆÙ‚Øª Ø§Ø¶Ø§ÙÙ‡", "Ø¨ÛŒÙ† Ø¯Ùˆ Ù†ÛŒÙ…Ù‡", "ØªØ§ÛŒÙ… Ø§ÙˆØª"]:
        return []
    
    odds_match = next((m for m in odds_data if (m["home_team"], m["away_team"]) == (match["team1"], match["team2"])), None)
    if not odds_match:
        return []
    
    alerts = []
    try:
        home_odds = float(odds_match["odds"]["home_win"]) if odds_match["odds"]["home_win"] != "N/A" else float("inf")
        away_odds = float(odds_match["odds"]["away_win"]) if odds_match["odds"]["away_win"] != "N/A" else float("inf")
        score1 = int(match["score"]["team1"]) if match["score"]["team1"].isdigit() else 0
        score2 = int(match["score"]["team2"]) if match["score"]["team2"].isdigit() else 0
        minute = match["minute"]
        
        base_minute = 30 if not minute or not minute.strip() else int(minute.split("+")[0])
        
        # Ø´Ø±Ø· 1: Ø¯Ù‚ÛŒÙ‚Ù‡ 60+ Ùˆ ØªÛŒÙ… Ø¨Ø§ Ø¶Ø±ÛŒØ¨ Ù¾Ø§ÛŒÛŒÙ† Ø¹Ù‚Ø¨ Ø§Ø³Øª
        if base_minute >= 60:
            if home_odds <= 1.6 and score1 < score2:
                alerts.append(generate_alert(match, home_odds, away_odds, "team1", match["team1"], match["team2"], home_odds, score1, score2, minute))
            if away_odds <= 1.6 and score2 < score1:
                alerts.append(generate_alert(match, home_odds, away_odds, "team2", match["team2"], match["team1"], away_odds, score2, score1, minute))
        
        # Ø´Ø±Ø· 2: Ø¨ÛŒÙ† Ø¯Ùˆ Ù†ÛŒÙ…Ù‡ Ùˆ Ù…Ø³Ø§ÙˆÛŒ
        if match["status"] == "Ø¨ÛŒÙ† Ø¯Ùˆ Ù†ÛŒÙ…Ù‡" and score1 == score2:
            if home_odds <= 1.6 or away_odds <= 1.6:
                circle1 = get_circle_color(min(home_odds, away_odds))
                circle2 = "ğŸŸ¢" if score1 == 0 and score2 == 0 else "ğŸŸ¡"
                alerts.append(
                    f"{circle1}{circle2} Ù‡Ø´Ø¯Ø§Ø±: Ø¯Ø± Ú©Ø´ÙˆØ± {match['country']} Ø¯Ø± Ù„ÛŒÚ¯ {match['league']} "
                    f"Ù…Ø³Ø§Ø¨Ù‚Ù‡ Ø¨ÛŒÙ† {match['team1']} (Ø¶Ø±ÛŒØ¨: {home_odds}) Ùˆ {match['team2']} (Ø¶Ø±ÛŒØ¨: {away_odds}) "
                    f"Ø¯Ø± Ù†ÛŒÙ…Ù‡ Ø§ÙˆÙ„ Ø¨Ø§ Ù†ØªÛŒØ¬Ù‡ {score1}-{score2} Ù…Ø³Ø§ÙˆÛŒ Ø§Ø³Øª!\n"
                    f"ğŸ“ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: 1_Ú¯Ù„ Ø¯Ø§Ø´ØªÙ† Ø¨Ø§Ø²ÛŒ 2_Ú¯Ù„ Ø²Ø¯Ù† ØªÛŒÙ… Ù‚ÙˆÛŒ"
                )
        
        # Ø´Ø±Ø· 3: Ø¨ÛŒÙ† Ø¯Ùˆ Ù†ÛŒÙ…Ù‡ØŒ Ø¶Ø±ÛŒØ¨ Ú©Ù…ØªØ± Ø§Ø² 1.4ØŒ Ù…Ø³Ø§ÙˆÛŒ ÛŒØ§ Ø¹Ù‚Ø¨ Ø§Ø³Øª
        if match["status"] == "Ø¨ÛŒÙ† Ø¯Ùˆ Ù†ÛŒÙ…Ù‡":
            checks = [
                ("team1", home_odds, score1, score2, match["team1"], match["team2"], away_odds),
                ("team2", away_odds, score2, score1, match["team2"], match["team1"], home_odds),
            ]
            for team_key, team_odd, team_score, opp_score, team_name, opp_name, opp_odd in checks:
                if team_odd < 1.4 and team_score <= opp_score:
                    circle = "ğŸŸ¢" if team_score < opp_score else "ğŸŸ "
                    status_desc = "Ø¹Ù‚Ø¨ Ø§Ø³Øª" if team_score < opp_score else "Ù…Ø³Ø§ÙˆÛŒ Ø§Ø³Øª"
                    alerts.append(
                        f"{circle} Ù‡Ø´Ø¯Ø§Ø±: Ø¯Ø± Ú©Ø´ÙˆØ± <b>{match['country']}</b> Ø¯Ø± Ù„ÛŒÚ¯ <b>{match['league']}</b> "
                        f"{team_name} (Ø¶Ø±ÛŒØ¨: {team_odd}) Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª <b>{match['status']}</b> "
                        f"Ø¨Ø§ Ù†ØªÛŒØ¬Ù‡ {team_score}-{opp_score} {status_desc}!\n"
                        f"ğŸ“ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Ú¯Ù„ Ø¯Ø§Ø´ØªÙ† Ø¨Ø§Ø²ÛŒ ØªØ§ Ø¯Ù‚ÛŒÙ‚Ù‡ Û·Û°"
                    )
    except (ValueError, KeyError) as e:
        logging.warning(f"Error processing match data: {e}")
    
    return alerts

# ============================================================
# Job Ù‡Ø§
# ============================================================
def scrape_odds_job():
    """ÙˆØ¸ÛŒÙÙ‡ Ø§Ø³Ú©Ø±Ù¾ Ø¶Ø±Ø§ÛŒØ¨"""
    logging.info("\n" + "="*60 + "\nğŸ² Starting ODDS job\n" + "="*60)
    driver = None
    try:
        driver = setup_driver()
        odds = scrape_odds(driver, "https://m.betforward.com/fa/sports/pre-match/event-view/Soccer?specialSection=upcoming-matches")
        if odds:
            logging.info(f"ğŸ“Š Retrieved {len(odds)} odds")
            update_odds_file(odds)
            logging.info("âœ… Odds updated")
        else:
            logging.warning("âš ï¸ No odds retrieved")
            asyncio.run(send_error_alert("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¶Ø±Ø§ÛŒØ¨\n\nØ³ÛŒØ³ØªÙ… Ù†ØªÙˆØ§Ù†Ø³Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¶Ø±Ø§ÛŒØ¨ Ø±Ø§ Ø§Ø² Ø³Ø§ÛŒØª betforward Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†Ø¯."))
    except Exception as e:
        error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¶Ø±Ø§ÛŒØ¨\n\nØ¬Ø²Ø¦ÛŒØ§Øª: {str(e)}"
        logging.error(f"âŒ Error in odds job: {e}")
        asyncio.run(send_error_alert(error_msg))
    finally:
        if driver:
            driver.quit()
        logging.info("ğŸ Odds job completed\n")

def scrape_results_job():
    """ÙˆØ¸ÛŒÙÙ‡ Ø§Ø³Ú©Ø±Ù¾ Ù†ØªØ§ÛŒØ¬"""
    logging.info("\n" + "="*60 + "\nâš½ Starting RESULTS job\n" + "="*60)
    driver = None
    try:
        driver = setup_driver()
        results = scrape_results(driver, "https://m.betforward.com/fa/sports/live/event-view/Soccer")
        if results:
            logging.info(f"ğŸ“Š Retrieved {len(results)} live matches")
            odds_data = load_json("betforward_odds.json")
            
            alerts = []
            for match in results:
                alerts.extend(check_alerts(match, odds_data))
            
            if alerts:
                logging.info(f"\nğŸš¨ Generated {len(alerts)} alerts")
                asyncio.run(send_alerts(alerts))
                logging.info("\nâœ… All alerts sent")
            else:
                logging.info("â„¹ï¸ No alerts generated")
            
            update_results_file(results)
            logging.info("âœ… Results updated")
        else:
            logging.warning("âš ï¸ No results retrieved")
            asyncio.run(send_error_alert("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù†ØªØ§ÛŒØ¬ Ø²Ù†Ø¯Ù‡\n\nØ³ÛŒØ³ØªÙ… Ù†ØªÙˆØ§Ù†Ø³Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†ØªØ§ÛŒØ¬ Ø²Ù†Ø¯Ù‡ Ø±Ø§ Ø§Ø² Ø³Ø§ÛŒØª betforward Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†Ø¯."))
    except Exception as e:
        error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù†ØªØ§ÛŒØ¬ Ø²Ù†Ø¯Ù‡\n\nØ¬Ø²Ø¦ÛŒØ§Øª: {str(e)}"
        logging.error(f"âŒ Error in results job: {e}")
        asyncio.run(send_error_alert(error_msg))
    finally:
        if driver:
            driver.quit()
        logging.info("ğŸ Results job completed\n")

def run_schedule():
    """Ø§Ø¬Ø±Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯"""
    schedule.every(20).minutes.do(scrape_odds_job)
    schedule.every(5).minutes.do(scrape_results_job)
    logging.info("\n" + "="*60 + "\nâ° Scheduler started\nğŸ“… Odds: Every 20min | Results: Every 5min\n" + "="*60 + "\n")
    while True:
        schedule.run_pending()
        time.sleep(60)

# ============================================================
# Ù†Ù‚Ø·Ù‡ ÙˆØ±ÙˆØ¯ Ø¨Ø±Ù†Ø§Ù…Ù‡
# ============================================================
if __name__ == "__main__":
    try:
        logging.info("\n" + "#"*60 + "\n# ğŸ¤– RoboBet Started ğŸ¤–\n" + "#"*60 + "\n")
        scrape_odds_job()
        scrape_results_job()
        run_schedule()
    except KeyboardInterrupt:
        logging.info("\n\nğŸ›‘ Stopped by user")
    except Exception as e:
        logging.error(f"\n\nâŒ Fatal error: {e}")
